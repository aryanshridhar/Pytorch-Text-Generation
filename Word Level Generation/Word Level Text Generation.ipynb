{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import torch.nn as nn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.max_rows = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetText: # class to extract text from the .txt file passed\n",
    "    def __init__(self , text_file):\n",
    "        \n",
    "        with open(text_file , 'r') as file:\n",
    "            self.text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4573337\n"
     ]
    }
   ],
   "source": [
    "data = GetText('Dataset.txt')\n",
    "text = data.text\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessing: # Only retains the non digit/number characters\n",
    "    def __init__(self , text):\n",
    "        self.text = text.lower()\n",
    "        \n",
    "        self.new_para = []\n",
    "        for i in nltk.word_tokenize(self.text):\n",
    "            self.new_para.append(re.sub('[^a-zA-Z]', \"\" , i))\n",
    "            \n",
    "        \n",
    "        \n",
    "        self.text = \" \".join(self.new_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = TextPreprocessing(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4531602"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = t1.text\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "849445\n"
     ]
    }
   ],
   "source": [
    "word_list = nltk.word_tokenize(data)\n",
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dup(seq): # Removes duplicate words from the dataset passed\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = remove_dup(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25495"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = {}\n",
    "for i in range(len(word_list)):\n",
    "    if word_list[i] not in word_dict:\n",
    "        word_dict[word_list[i]] = i\n",
    "        \n",
    "\n",
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_text = [word_dict[w] for w in word_dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenRNN(nn.Module):\n",
    "    def __init__(self , input_size , hidden_size , output_size , n_layers):\n",
    "        \n",
    "        super(TextGenRNN , self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.input_size, self.hidden_size)\n",
    "        self.rnn = nn.LSTM(self.hidden_size , self.hidden_size , batch_first = True)\n",
    "        self.linear = nn.Linear(self.hidden_size , self.output_size)\n",
    "        \n",
    "    def forward(self , batch):\n",
    "        batch = batch.long()\n",
    "        out = self.embedding(batch)\n",
    "        out , _ = self.rnn(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gen Class for LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generation:\n",
    "    def __init__(self, int_text , word_dict):\n",
    "        \n",
    "\n",
    "        self.seq_len = 32 # predicting next word form the previous 32 words\n",
    "        self.batch_size = 16 # total of 16 , 32 seq in a batch\n",
    "        \n",
    "        self.word_dict = {i:j for j,i in word_dict.items()}\n",
    "        \n",
    "        self.int_text = int_text[:-(len(int_text) % (self.seq_len * self.batch_size))] # for forming excat batch sizes\n",
    "        \n",
    "        self.input_size = int_text[-1]\n",
    "        self.hidden_size = 256\n",
    "        self.output_size = self.input_size\n",
    "        self.n_layers = 1\n",
    "        self.epochs = 50\n",
    "        self.print_every = 2\n",
    "        self.lr = 0.001\n",
    "        \n",
    "        self.rnn = TextGenRNN(self.input_size , \n",
    "                              self.hidden_size , \n",
    "                              self.output_size , \n",
    "                              self.n_layers)\n",
    "        \n",
    "\n",
    "    def get_batches(self):\n",
    "    \n",
    "        i = 0\n",
    "        counter = 0    \n",
    "        no_time = len(self.int_text) // (self.seq_len * self.batch_size)\n",
    "        \n",
    "        while i != int(no_time)-1:\n",
    "            \n",
    "            x_list = []\n",
    "            y_seq = []\n",
    "            \n",
    "            for _ in range(self.batch_size):\n",
    "                x_list.append(self.int_text[counter:self.seq_len+counter])\n",
    "                y_seq.append(self.int_text[counter + 1:self.seq_len+counter+1])\n",
    "                counter += self.seq_len\n",
    "            \n",
    "            i += 1\n",
    "            yield x_list,y_seq\n",
    "            \n",
    "            \n",
    "    def show_batches(self):\n",
    "        for i,j in self.get_batches():\n",
    "            i , j = torch.Tensor(i).long() , torch.Tensor(j).long()\n",
    "            print(i)\n",
    "            print(j)\n",
    "            break\n",
    "            \n",
    "    def init_loss(self):\n",
    "        \n",
    "        self.rnn = TextGenRNN(self.input_size , \n",
    "                              self.hidden_size , \n",
    "                              self.output_size , \n",
    "                              self.n_layers)\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            for label,actual in self.get_batches():\n",
    "                    \n",
    "                label = torch.Tensor(label).float()\n",
    "                actual = torch.Tensor(actual).long()\n",
    "            \n",
    "                y_pred = self.rnn(label)\n",
    "                y_pred = y_pred.transpose(1,2)\n",
    "                loss = self.criterion(y_pred,actual)\n",
    "                \n",
    "                print(loss.item())\n",
    "                break\n",
    "            break\n",
    "            \n",
    "    def train(self):\n",
    "        \n",
    "        self.iterations = []\n",
    "        self.loss_ = []\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.rnn.parameters() , lr = self.lr)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            for label,actual in self.get_batches():\n",
    "\n",
    "                    \n",
    "                label = torch.Tensor(label).float()\n",
    "                actual = torch.Tensor(actual).long()\n",
    "            \n",
    "                y_pred = self.rnn(label)\n",
    "                y_pred = y_pred.transpose(1,2)\n",
    "                \n",
    "                loss = self.criterion(y_pred,actual)\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "            self.iterations.append(epoch)\n",
    "            self.loss_.append(loss)\n",
    "            \n",
    "            if epoch % self.print_every == 0:\n",
    "                print(f\"Loss after {epoch} iteration : {loss}\")\n",
    "                \n",
    "                \n",
    "        self.show_plot(self.iterations , self.loss_)\n",
    "                \n",
    "                \n",
    "    def show_plot(self,x,y):\n",
    "        plt.plot(x,y)\n",
    "        plt.xlabel('Epoch/Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def generate_text(self , init_str = 'if they would',predict_len = 200):\n",
    "        \n",
    "        generated_list = []\n",
    "        \n",
    "        for _ in range(predict_len):\n",
    "            encoded_input = torch.Tensor([word_dict[w] for w in init_str.lower().split(' ')]).long().unsqueeze(0)\n",
    "            gen_text = self.rnn(encoded_input)\n",
    "            _ , max_ele = torch.topk(gen_text[0] , k = 1)\n",
    "            generated_list.append(self.word_dict[max_ele[0].item()])\n",
    "            \n",
    "            init_str = self.word_dict[max_ele[0].item()]\n",
    "            \n",
    "            \n",
    "        return \" \".join(generated_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = Generation(int_text , word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "          28,  29,  30,  31],\n",
      "        [ 32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  45,\n",
      "          46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,\n",
      "          60,  61,  62,  63],\n",
      "        [ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "          78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "          92,  93,  94,  95],\n",
      "        [ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "         110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "         124, 125, 126, 127],\n",
      "        [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "         142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "         156, 157, 158, 159],\n",
      "        [160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173,\n",
      "         174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187,\n",
      "         188, 189, 190, 191],\n",
      "        [192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "         206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "         220, 221, 222, 223],\n",
      "        [224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "         238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "         252, 253, 254, 255],\n",
      "        [256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
      "         270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "         284, 285, 286, 287],\n",
      "        [288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301,\n",
      "         302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315,\n",
      "         316, 317, 318, 319],\n",
      "        [320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333,\n",
      "         334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347,\n",
      "         348, 349, 350, 351],\n",
      "        [352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365,\n",
      "         366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379,\n",
      "         380, 381, 382, 383],\n",
      "        [384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
      "         398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
      "         412, 413, 414, 415],\n",
      "        [416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429,\n",
      "         430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443,\n",
      "         444, 445, 446, 447],\n",
      "        [448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
      "         462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
      "         476, 477, 478, 479],\n",
      "        [480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
      "         494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507,\n",
      "         508, 509, 510, 511]])\n",
      "tensor([[  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
      "          15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
      "          29,  30,  31,  32],\n",
      "        [ 33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  45,  46,\n",
      "          47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,\n",
      "          61,  62,  63,  64],\n",
      "        [ 65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
      "          79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,\n",
      "          93,  94,  95,  96],\n",
      "        [ 97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110,\n",
      "         111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124,\n",
      "         125, 126, 127, 128],\n",
      "        [129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "         143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
      "         157, 158, 159, 160],\n",
      "        [161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174,\n",
      "         175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188,\n",
      "         189, 190, 191, 192],\n",
      "        [193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206,\n",
      "         207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "         221, 222, 223, 224],\n",
      "        [225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
      "         239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252,\n",
      "         253, 254, 255, 256],\n",
      "        [257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270,\n",
      "         271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284,\n",
      "         285, 286, 287, 288],\n",
      "        [289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302,\n",
      "         303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316,\n",
      "         317, 318, 319, 320],\n",
      "        [321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334,\n",
      "         335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348,\n",
      "         349, 350, 351, 352],\n",
      "        [353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366,\n",
      "         367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380,\n",
      "         381, 382, 383, 384],\n",
      "        [385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398,\n",
      "         399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412,\n",
      "         413, 414, 415, 416],\n",
      "        [417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430,\n",
      "         431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444,\n",
      "         445, 446, 447, 448],\n",
      "        [449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462,\n",
      "         463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476,\n",
      "         477, 478, 479, 480],\n",
      "        [481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494,\n",
      "         495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508,\n",
      "         509, 510, 511, 512]])\n"
     ]
    }
   ],
   "source": [
    "g1.show_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.153639793395996\n"
     ]
    }
   ],
   "source": [
    "g1.init_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iteration : 10.152101516723633\n",
      "Loss after 2 iteration : 8.500555992126465\n",
      "Loss after 4 iteration : 5.131058692932129\n",
      "Loss after 6 iteration : 2.3446688652038574\n",
      "Loss after 8 iteration : 0.6937149167060852\n",
      "Loss after 10 iteration : 0.2676803767681122\n",
      "Loss after 12 iteration : 0.138881653547287\n",
      "Loss after 14 iteration : 0.09104717522859573\n",
      "Loss after 16 iteration : 0.06797119230031967\n",
      "Loss after 18 iteration : 0.05377395451068878\n",
      "Loss after 20 iteration : 0.04401811957359314\n",
      "Loss after 22 iteration : 0.036893557757139206\n",
      "Loss after 24 iteration : 0.031470321118831635\n",
      "Loss after 26 iteration : 0.027213547378778458\n",
      "Loss after 28 iteration : 0.023791925981640816\n",
      "Loss after 30 iteration : 0.02098863758146763\n",
      "Loss after 32 iteration : 0.018655704334378242\n",
      "Loss after 34 iteration : 0.016688598319888115\n",
      "Loss after 36 iteration : 0.015011390671133995\n",
      "Loss after 38 iteration : 0.013567591086030006\n",
      "Loss after 40 iteration : 0.01231429073959589\n",
      "Loss after 42 iteration : 0.011218366213142872\n",
      "Loss after 44 iteration : 0.010253814049065113\n",
      "Loss after 46 iteration : 0.009399986825883389\n",
      "Loss after 48 iteration : 0.008640233427286148\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAc3UlEQVR4nO3dfXAc9Z3n8fd3ZvRgyXq0ZAksy5KDHwPGBtmGkOwRMAnJ5kKSzeURjmyopTaXXVg2u3vs3lVld6tyxd4Deagkd8fyEAiE7CaEhOS2Eh4SNmwSjGWwDdg82sbPtvwgS7YsSxp974/psUayZMYPMz0z/XlVTU33b3q6vw3yR61fd//a3B0REYmOWNgFiIhIfin4RUQiRsEvIhIxCn4RkYhR8IuIREwi7AKy0dTU5B0dHWGXISJSVNauXbvf3ZsnthdF8Hd0dNDd3R12GSIiRcXM3pqsXV09IiIRo+AXEYkYBb+ISMQo+EVEIkbBLyISMQp+EZGIUfCLiERMSQf/U5v28oPu7WGXISJSUHIW/GZ2r5ntM7OXMtoazewJM3s9eG/I1fbdnYdWb+M/P7KBn7+0O1ebEREpOrk84v8OcO2EttuBp9x9HvBUMJ8TZsY3P7OMi2fXc8vD6/jtG/tztSkRkaKSs+B3918DByc0XwfcH0zfD3wkV9sHqCpPcN/nltPZVM0fPdDN+u29udyciEhRyHcff4u77wYI3mdOtaCZ3Wxm3WbW3dPTc8YbrK8q54GbVtBQXc7n7nuON/b1n/G6RERKQcGe3HX3u9y9y927mptPGlzutLTUVvLgTSuJx2LccM9z7Ow9do6qFBEpPvkO/r1mdh5A8L4vXxvuaKrmgc+v4MjxEW64ZzUHjhzP16ZFRApKvoP/MeDGYPpG4Cf53Pji82u593PL2dV7jM/f342753PzIiIFIZeXcz4M/A5YYGY7zOwm4A7gGjN7HbgmmM+r5R2N3H7tQtZv7+XNniP53ryISOhy9iAWd//0FB9dnattZuvfLZgJP93I6i0HuWBmTdjliIjkVcGe3M2ljhlVNNdU8NyWiVebioiUvkgGv5mxsrOR1ZsPqp9fRCInksEPsLKzkT19g2w/qEs7RSRaIhv8KzpnAPDslgMhVyIikl+RDf55M6fTUFWmfn4RiZzIBn8sZizvaFTwi0jkRDb4AVbOncG2gwPsPqx+fhGJjmgHf2cjgI76RSRSIh38i86rpaYiwbObFfwiEh2RDv54zOjqaOA5XdkjIhES6eCH1GWdb/YcZb9G6xSRiIh88K+cq35+EYmWyAf/RbPqmFYWV/CLSGREPvjL4jEundPAs5vVzy8i0RD54AdY0dnIq3v76R0YCrsUEZGcU/CTup7fHdZsPRR2KSIiOafgBy6eXU95IqbLOkUkEhT8QGVZnKWz63WCV0QiQcEfWNnZyEu7+jhyfCTsUkREckrBH1jR2Uhy1Fn7lvr5RaS0KfgDl85pIBEzVuuyThEpcQr+QFV5ggtn1amfX0RKnoI/w8q5jazf0cvgcDLsUkREckbBn2H5nEaGk86GHYfDLkVEJGcU/BkWn18LwKt7+kKuREQkdxT8Gc6rq6SmMsEre/rDLkVEJGcU/BnMjIWtNbyq4BeREqbgn2BBaw2v7u3H3cMuRUQkJxT8EyxoraV/cIRdhwfDLkVEJCcU/BMsbK0BdIJXREpXKMFvZreZ2ctm9pKZPWxmlWHUMZn5Lang1wleESlVeQ9+M5sF3AJ0ufuFQBz4VL7rmErdtDLOr6vUCV4RKVlhdfUkgGlmlgCqgF0h1TGpBbqyR0RKWN6D3913Av8T2AbsBg67++MTlzOzm82s28y6e3p68lrjgtZa3uw5wnByNK/bFRHJhzC6ehqA64BO4Hyg2syun7icu9/l7l3u3tXc3JzXGhe21jCcdLbsP5rX7YqI5EMYXT2rgC3u3uPuw8CPgHeFUMeUFrTqBK+IlK4wgn8bcJmZVZmZAVcDm0KoY0rvaJ5OIma6pFNESlIYffyrgR8CzwMvBjXcle86TqU8EWNuc7VO8IpISUqEsVF3/zLw5TC2na35LTWs294bdhkiIuec7tydwsLWGnYcOqaHr4tIyVHwT2FBa3psfnX3iEhpUfBPYWzMHgW/iJQWBf8UZtVPo7o8rit7RKTkKPinEIsZ81trdC2/iJQcBf8pLNRDWUSkBCn4T2FBSw29A8Ps6z8edikiIueMgv8U0lf2qLtHREqJgv8U9DQuESlFCv5TaKguZ2ZNhY74RaSkKPjfxoLWGl7bq+AXkdKh4H8bC1treH3vEZKjurJHREqDgv9tLGit5fjIKFsP6KEsIlIaFPxvQ0M3iEipUfC/jQtmTidmuqRTREqHgv9tVJbF6Wiq1iWdIlIyFPxZWNBSo64eESkZCv4sLGit4a2DAwwM6aEsIlL8FPxZWNhagzu8vvdI2KWIiJw1BX8WxsbsUT+/iBQ/BX8W5jRWMb0iwUs7FfwiUvwU/FmIxYwLZ9WyYefhsEsRETlrCv4sXdxWz6ZdfQyNjIZdiojIWVHwZ+mitjqGkqMasE1Eip6CP0sXt9UDsH5Hb8iViIicHQV/ltoaptFQVcaLO9TPLyLFTcGfJTPjorZ61iv4RaTIKfhPw5JZdby2t59jQ8mwSxEROWMK/tOwpK2O5Kizcbeu5xeR4qXgPw1LghO8G3SCV0SKWCjBb2b1ZvZDM3vFzDaZ2eVh1HG6WusqmVlToRO8IlLUEiFt9+vAz93942ZWDlSFVMdpW9JWr0s6RaSo5f2I38xqgd8D7gFw9yF3L5okXdJWx+b9R+kfHA67FBGRMxJGV89coAe4z8xeMLO7zax64kJmdrOZdZtZd09PT/6rnMKStjrc0YBtIlK0wgj+BHAJ8L/dfRlwFLh94kLufpe7d7l7V3Nzc75rnFL6BO+LO4vmjxQRkXHCCP4dwA53Xx3M/5DUL4Ki0FhdTlvDNN3IJSJFK+/B7+57gO1mtiBouhrYmO86zsaStjpd2SMiRSus6/j/FHjIzDYAS4H/FlIdZ2RJWz3bDg5w6OhQ2KWIiJy2UC7ndPd1QFcY2z4XlsyqA+DFnYf5vfmFc/5BRCQbunP3DFzYlgp+3cErIsVIwX8GaivLmNtUzQb184tIEVLwn6ElbXUKfhEpSgr+M3RRWz17+gbZ1zcYdikiIqclq+A3s3eYWUUwfaWZ3WJm9bktrbBdfKKfX0f9IlJcsj3ifwRImtkFpMbY6QS+l7OqisA7z68jZrBhp4JfRIpLtsE/6u4jwEeBr7n7bcB5uSur8E0rjzO/pUZX9ohI0ck2+IfN7NPAjcDPgray3JRUPNJ38Lp72KWIiGQt2+D/Q+By4CvuvsXMOoEHc1dWcbiorZ4DR4fY2Xss7FJERLKW1Z277r4RuAXAzBqAGne/I5eFFYP0Cd4XdxymraFoniUjIhGX7VU9T5tZrZk1AutJjaV/Z25LK3wLWmsoi5tG6hSRopJtV0+du/cBHwPuc/dLgVW5K6s4VCTiLD6vlvXbdYJXRIpHtsGfMLPzgE8wdnJXgKWzU8/gTY7qBK+IFIdsg//vgV8Ab7r7GjObC7yeu7KKx7L2BgaGkry2tz/sUkREspJV8Lv7D9x9ibt/IZjf7O5/kNvSisOy9tQNzOvU3SMiRSLbk7ttZvaome0zs71m9oiZteW6uGLQ3lhFY3U5L2w7FHYpIiJZybar5z7gMeB8YBbw06At8syMpbPreWGbjvhFpDhkG/zN7n6fu48Er+8AevRUYNnset7oOULf4HDYpYiIvK1sg3+/mV1vZvHgdT1wIJeFFZOl7fW4w4btup5fRApftsH/eVKXcu4BdgMfJzWMgwAXz67HDPXzi0hRyPaqnm3u/mF3b3b3me7+EVI3cwmpRzFe0DxdV/aISFE4mydw/fk5q6IELJ1dzwvbezVSp4gUvLMJfjtnVZSAZe0NHDw6xLaDA2GXIiJySmcT/Dq0zZC+kUuXdYpIoTtl8JtZv5n1TfLqJ3VNvwTmt9RQVR5XP7+IFLxTjsfv7jX5KqTYxWPGkrY6XdkjIgXvbLp6ZIJl7Q1s3N3H4HAy7FJERKak4D+Hls2uZzjpvLyrL+xSRESmpOA/h5aeOMGr7h4RKVwK/nNoZk0ls+qn8YJO8IpIAQst+IMxf14ws5J6otey9nrW6ZJOESlgYR7x3wpsCnH7ObF0dj07e4+xr28w7FJERCYVSvAHD3H5feDuMLafS8vaGwDU3SMiBSusI/6vAX8FjE61gJndbGbdZtbd09OTv8rO0jvPr6UsbrqDV0QKVt6D38w+BOxz97WnWs7d73L3Lnfvam4unme+VJbFWXxeLeu268oeESlMYRzxXwF82My2At8HrjKzB0OoI2eWtTewYcdhRpJT/kEjIhKavAe/u/+1u7e5ewfwKeCX7n59vuvIpWXt9QwMJXlt75GwSxEROYmu48+BpbNTN3JpwDYRKUShBr+7P+3uHwqzhlxob6yisbpcd/CKSEHSEX8OmBnLZtfzvIJfRAqQgj9HujoaebPnKAeOHA+7FBGRcRT8ObKiM3Uj15qtOuoXkcKi4M+Ri2bVU5GI8dyWg2GXIiIyjoI/R8oTMS5pb+C5rQfCLkVEZBwFfw6t6Gxk464++geHwy5FROQEBX8OrexsZNRh7Vvq5xeRwqHgz6Fl7Q0kYqZ+fhEpKAr+HJpWHueitjoFv4gUFAV/jq3obGT9jl4Gh5NhlyIiAij4c25lZyPDSde4PSJSMBT8OXbpnEbMUHePiBQMBX+O1U0rY2FrrYJfRAqGgj8PVnY2svatQwzrwSwiUgAU/HmworORY8NJXtp5OOxSREQU/PmwvKMRgDVb1d0jIuFT8OdBc00Fc5uq1c8vIgVBwZ8nKzobeW7LQUZHPexSRCTiFPx5sqKzkb7BEV7d2x92KSIScQr+PFnRqX5+ESkMCv48aWuo4vy6Slarn19EQqbgz6N0P7+7+vlFJDwK/jxa0TmDnv7jbD0wEHYpIhJhCv48OtHPr+4eEQmRgj+P3tFczYzqcvXzi0ioFPx5ZGYs72jUA9hFJFQK/jxbObeR7QePsU39/CISEgV/nl21cCYAT27aG3IlIhJVCv48mzOjmvkt0xX8IhIaBX8IVi1qYfWWgxweGA67FBGJoLwHv5nNNrNfmdkmM3vZzG7Ndw1hW7W4heSo8/Rr+8IuRUQiKIwj/hHgS+6+CLgM+KKZLQ6hjtAsbaunaXo5T25S8ItI/uU9+N19t7s/H0z3A5uAWfmuI0yxmHH1whaefnUfQyN6HKOI5Feoffxm1gEsA1ZP8tnNZtZtZt09PT35Li3nVi1uoX9wRA9nEZG8Cy34zWw68AjwZ+7eN/Fzd7/L3bvcvau5uTn/BebYuy9oorIspqt7RCTvQgl+MysjFfoPufuPwqghbNPK47z7gmae2LhXo3WKSF6FcVWPAfcAm9z9znxvv5Bcs3gmO3uP8coePZVLRPInjCP+K4AbgKvMbF3w+mAIdYTuqoUtmMGTG9XdIyL5k8j3Bt393wDL93YLUXNNBUtn1/Pkpr386dXzwi5HRCJCd+6GbNWiFtbvOMzevsGwSxGRiFDwh+yaxS0APKWbuUQkTxT8IZs3czrtjVU8sXFP2KWISEQo+ENmZqxa1MJv3jzA0eMjYZcjIhGg4C8AqxbPZGhklGde3x92KSISAQr+ArC8o5HayoTu4hWRvFDwF4CyeIz3LpzJL1/ZR3JUd/GKSG4p+AvEqkUtHDw6xO/e1IPYRSS3FPwF4prFLbTUVvDVJ1/T2D0iklMK/gJRWRbnlqvnsfatQ/zqVV3TLyK5o+AvIJ/oms2cGVX8j1+8xqj6+kUkRxT8BaQsHuPPr5nPpt19/OzF3WGXIyIlSsFfYP79kvNZ2FrDnY+/ynBSj2UUkXNPwV9gYjHjL963gK0HBvhB946wyxGREqTgL0BXL5rJJe31fOOp1xkcToZdjoiUGAV/ATIz/vL9C9nTN8h3f/dW2OWISIlR8Beoy98xg/fMa+LbT79B/+Bw2OWISAlR8Bewv3z/Ag4NDHP3M1vCLkVESoiCv4AtaavnAxe2cvczmzl4dCjsckSkRCj4C9yX3jefY8NJ/vjBtfQOKPxF5Owp+AvcBTNr+Oonl7JuWy8f+/Zv2br/aNgliUiRU/AXgeuWzuKhP1rJoYEhPvrt37Bm68GwSxKRIqbgLxLLOxp59D9dQX1VOZ/9x9X8ZN3OsEsSkSKl4C8iHU3V/OgL72Jpez23fn8d33jqdQ3hLCKnTcFfZBqqy/nuTSv42LJZ3PnEa3z+O2tYv7037LJEpIgo+ItQRSLO//rExfyXDy7i+W29XPet33DDPat5dvMB/QUgIm/LiiEourq6vLu7O+wyCtKR4yM8+Oxb3P3MZvYfGaJrTgNffO8FXLmgGTMLuzwRCZGZrXX3rpPaFfylYXA4yT+t2c7//dc32XV4kLlN1bx7XhOXz53ByrkzaKwuD7tEEckzBX9EDI2M8uN1O/nZht10bz3IwFBqdM+FrTVcNncGKzobmTdzOu0zqqhIxEOuVkRyScEfQcPJUTbsOMyzmw/w7OYDrNl6kMHh1MNdYgazG6uY21TN3ObpdDZV01pbSVNNBc01FTRNL9cvBpEiV1DBb2bXAl8H4sDd7n7HqZZX8J8bQyOjvLKnj809R9ncc4Q39x9lc89Rtuw/cuIXQqbaygRNNRXUTyujprKMmsoENZVl1FYmqKlMUF2RoKo8TmVZnKryselpZXEqymJUJGJUJMamy+MxnXcQyaOpgj8RQiFx4FvANcAOYI2ZPebuG/NdS9SUJ2IsaatnSVv9uPbRUWdv/yD7+o7T03+c/UdSr9T0EIePDXNoYIhtBwfoHxymb3CEoZEzeyxkeTxGWdwoT8Qoi6de5YkYiZgF80YiPjafiBuJmBGPjbXHY2Nt8ZgRNyMeSy0bMyMeg7gZseCzWMZysZgRM4jH0sum5s3Sn0PMbGw++CxmqfZYLD0ffMb4Zcbex5Yz0usM2klNn2gjtWx6eqw94zvBesa1A0yYn7gcJ7Y5+TrSJradtLx+YZeUvAc/sAJ4w903A5jZ94HrAAV/SGIx47y6aZxXNy3r7xwfSTJwPMnAcJJjQ8FrOMnA0AiDw0mOj4yOvdLzw0mGks5wcvTE6/jIKMNJZ+REmzMymnofGBohOeoMJ53kaKo99e6MJJ2kO6PBfPo9OZpqT44WfhdmsZr4yyE9nf7MGFtgsnYb1z72C8UyJjI/z9xe5ncy151R3aR1jp8f/0ts3C/ASeo6aZlxy5+8vfFVTLGPkzRM9Z17b1xO+4yqid88K2EE/yxge8b8DmDlxIXM7GbgZoD29vb8VCZZq0jEqUjEaQi7kFMYzfglMHrifaw9/e4OydHU+6h7xiuYH029pz9Pf8eDZTxjWXdwxtrdx3/XyWxPLZv6zsnLn2jL+Cw1Pf57nFgmc/mx7zPJd9I9vOk2Tlp2/HdSy459ccp1QMY6xhrH2k9eNmO149Y1rqZJlsucn2p9TFhuYs+2Z3xrsn2YuMwUk+Pun5m8jvHtp/rOxAXLE+f+dqswgn+yvxlPOjxz97uAuyDVx5/roqT0xGJGDKNM56hFxgnjzt0dwOyM+TZgVwh1iIhEUhjBvwaYZ2adZlYOfAp4LIQ6REQiKe9dPe4+YmZ/AvyC1OWc97r7y/muQ0QkqsLo48fd/wX4lzC2LSISdRqdU0QkYhT8IiIRo+AXEYkYBb+ISMQUxeicZtYDvHWGX28C9p/DcoqF9jtaorrfEN19z2a/57h788TGogj+s2Fm3ZONTlfqtN/REtX9huju+9nst7p6REQiRsEvIhIxUQj+u8IuICTa72iJ6n5DdPf9jPe75Pv4RURkvCgc8YuISAYFv4hIxJR08JvZtWb2qpm9YWa3h11PrpjZvWa2z8xeymhrNLMnzOz14L2QH5Z1Rsxstpn9ysw2mdnLZnZr0F7S+25mlWb2nJmtD/b774L2TjNbHez3PwXDnpccM4ub2Qtm9rNgvuT328y2mtmLZrbOzLqDtjP+OS/Z4M94qPsHgMXAp81scbhV5cx3gGsntN0OPOXu84CngvlSMwJ8yd0XAZcBXwz+H5f6vh8HrnL3i4GlwLVmdhnwD8BXg/0+BNwUYo25dCuwKWM+Kvv9XndfmnHt/hn/nJds8JPxUHd3HwLSD3UvOe7+a+DghObrgPuD6fuBj+S1qDxw993u/nww3U8qDGZR4vvuKUeC2bLg5cBVwA+D9pLbbwAzawN+H7g7mDcisN9TOOOf81IO/ske6j4rpFrC0OLuuyEVkMDMkOvJKTPrAJYBq4nAvgfdHeuAfcATwJtAr7uPBIuU6s/714C/AkaD+RlEY78deNzM1prZzUHbGf+ch/IgljzJ6qHuUvzMbDrwCPBn7t6XOggsbe6eBJaaWT3wKLBossXyW1VumdmHgH3uvtbMrkw3T7JoSe134Ap332VmM4EnzOyVs1lZKR/xR/2h7nvN7DyA4H1fyPXkhJmVkQr9h9z9R0FzJPYdwN17gadJneOoN7P0wVwp/rxfAXzYzLaS6rq9itRfAKW+37j7ruB9H6lf9Cs4i5/zUg7+qD/U/THgxmD6RuAnIdaSE0H/7j3AJne/M+Ojkt53M2sOjvQxs2nAKlLnN34FfDxYrOT2293/2t3b3L2D1L/nX7r7Zynx/TazajOrSU8D7wNe4ix+zkv6zl0z+yCpI4L0Q92/EnJJOWFmDwNXkhqmdS/wZeDHwD8D7cA24D+4+8QTwEXNzN4NPAO8yFif79+Q6ucv2X03syWkTubFSR28/bO7/72ZzSV1JNwIvABc7+7Hw6s0d4Kunr9w9w+V+n4H+/doMJsAvufuXzGzGZzhz3lJB7+IiJyslLt6RERkEgp+EZGIUfCLiESMgl9EJGIU/CIiEaPgl6JhZslgdML065wNvmZmHZmjm07y+eVm9o9mdmXGqJBXmtm7znENn8mY7zKzb5yr9YuklfKQDVJ6jrn70pC2fS3w8wltVwJHgN9muxIzS2SMKzNRB/AZ4HsA7t4NdJ9uoSJvR0f8UvSCscr/IRij/jkzuyBon2NmT5nZhuC9PWhvMbNHg/Hs12cctceDo/qXzezx4K7YtKuBJzO22QH8MXBb8NfHe4I7ah8xszXB64pg2b81s7vM7HHggeDI/hkzez54pbd/B/CeYH23TfjrotHMfhzsy7PBTVzpdd9rZk+b2WYzuyVX/52ldCj4pZhMm9DV88mMz/rcfQXwTVJ3axNMP+DuS4CHgHS3yTeAfw3Gs78EeDlonwd8y93fCfQCfwBgZk3AsLsfTm/M3bcC/4fUOPBL3f0Z4OvB/PLgu3dn1HcpcJ27f4bUmCrXuPslwCcz6rodeCZY31cn7PvfAS8E+/I3wAMZny0E3k9q/JYvB+MXiUxJXT1STE7V1fNwxns6NC8HPhZMfxf478H0VcB/hBOjXB4Onl60xd3XBcusJdX1AqmxUR7Por5VwOKM0UFr02OsAI+5+7Fgugz4ppktBZLA/CzW/W6CX0Tu/kszm2FmdcFn/y8YouC4me0DWkgNUigyKQW/lAqfYnqqZSaTOb5LEkh39XwAuPPkxU8SAy7PCHgAgl8ERzOabiM1ptLFwXcGs1j3qYYfnli3/l3LKamrR0rFJzPefxdM/5bUKI4AnwX+LZh+CvgCnHigSe1UKw1GAF0CrJvk436gJmP+ceBPMr471V8ndcBudx8FbiA12Npk68v062Af0gOU7Xf3vqnqFjkVBb8Uk4l9/HdkfFZhZqtJPY/1tqDtFuAPzWwDqYC9NWi/FXivmb1IqkvnnafY5qWk+tYn+2vhp8BH0yd3g+11BSdgN5I6+TuZbwM3mtmzpLp50n8NbABGghPOt034zt+m103qJPCNiJwhjc4pRS94MEeXu+/Pwbr/K6lnN3//XK9bJCwKfil6uQx+kVKk4BcRiRj18YuIRIyCX0QkYhT8IiIRo+AXEYkYBb+ISMT8f2ta2PvbhyCBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g1.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'they yield but superfluity while were wholesome might guess relieved humanely think too dear leanness that afflicts object of misery as an inventory particularise their abundance sufferance gain them revenge this with pikes ere become rakes for gods i in hunger bread not thirst especially against he s very dog commonalty consider services has his country well could content give report fort pays himself being proud nay maliciously say unto hath famously did end though softconscienced men can was please mother partly which even till altitude virtue help nature account vice must way covetous need barren accusations faults surplus tire'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1.generate_text(predict_len = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'they yield but superfluity while were wholesome might guess relieved humanely think too dear leanness that afflicts object of misery as an inventory particularise their abundance sufferance gain them revenge this with pikes ere become rakes for gods i in hunger bread not thirst especially against he s very dog commonalty consider services has his country well could content give report fort pays himself being proud nay maliciously say unto hath famously did end though softconscienced men can was please mother partly which even till altitude virtue help nature account vice must way covetous need barren accusations faults surplus tire repetition shouts these other side city risen why stay prating here capitol come soft who comes worthy menenius agrippa always loved honest enough rest so work my countrymen hand where go bats clubs matter pray business unknown senate had inkling fortnight intend do now show em deeds suitors strong breaths shall arms masters friends mine neighbours will undo yourselves sir undone already tell most charitable care your wants suffering dearth may strike heaven staves lift roman state whose course takes cracking ten thousand curbs link asunder ever appear impediment make knees alack transported by calamity thither attends slander helms like fathers when curse enemies true indeed neer cared yet suffer storehouses crammed grain edicts usury support usurers repeal daily act established rich provide piercing statutes chain up restrain wars eat there love bear either confess wondrous malicious or accused folly pretty tale heard since serves purpose venture stale t little fob off disgrace deliver time body members rebell d belly thus only gulf remain midst idle unactive still cupboarding viand never bearing labour instruments see devise instruct walk feel mutually participate minister appetite affection common whole answer made kind smile came from lungs look tauntingly replied discontented mutinous parts envied receipt fitly malign senators such kinglycrowned head vigilant eye counsellor heart arm soldier steed leg tongue trumpeter muniments petty helps fabric then fore fellow speaks should cormorant sink former agents complain bestow small patience awhile ye re long about note friend grave deliberate rash accusers incorporate quoth receive general food live upon fit because am storehouse shop remember send through rivers blood court seat brain cranks offices man strongest nerves inferior veins natural competency whereby once says mark ay out each audit back flour leave bran tot how apply rome examine counsels cares digest things rightly touching weal find public benefit proceeds great toe assembly lowest basest poorest wise rebellion thou gost foremost rascal art worst run leadst win some vantage ready stiff her rats point battle bale hail noble thanks dissentious rogues rubbing itch opinion scabs words thee flatter beneath abhorring curs nor peace war affrights makes trusts lions finds hares foxes geese surer coal fire ice hailstone sun offence subdues justice deserves greatness hate affections sick desires increase evil depends favours swims fins lead hews down oaks rushes hang trust every minute change mind call vile garland several places cry under keep awe else feed another seeking rates whereof stored sit presume rise thrives declines factions conjectural marriages making parties feebling stand liking below cobbled shoes theres nobility lay aside ruth use sword quarry thousands quarter slaves high pick lance almost thoroughly persuaded abundantly lack discretion passing cowardly beseech troop dissolved said anhungry sigh forth proverbs broke stone walls dogs meat mouths sent shreds vented complainings petition granted strange break generosity bold power pale threw caps horns moon shouting emulation five tribunes defend vulgar wisdoms choice junius brutus sicinius velutus sdeath rabble unroof prevail throw greater themes insurrection arguing get home fragments messenger news volsces glad ha means vent musty best elders senator lately told leader tullus aufidius put sin envying thing wish cominius fought together half world ears party ild revolt lion hunt attend promise constant titus lartius shalt face standst lean crutch fight tother behind truebred company greatest follow much take gnaw garners worshipful mutiners valour puts equal chosen lip eyes taunts moved spare gird bemock modest present devour grown valiant tickled success disdains shadow treads noon wonder insolence brook commanded fame aims whom graced better held attain place miscarries fault perform utmost giddy censure borne besides sticks demerits rob honours earned aught merit hence dispatch fashion singularity goes action lets along entered yours been thought brought bodily circumvention four days gone thence letter yes press known whether east west rumour old worse hated three preparation whither bent likely army field doubt pretences veil needs themselves hatching seem discovery shorten aim many towns afoot commission hie bands alone guard corioli set remove bring ve prepared certainties parcels hitherward chance meet tis sworn between assist safe farewell volumnia daughter sing express yourself comfortable sort son husband freelier rejoice absence wherein won honour embracements bed tenderbodied womb youth comeliness plucked gaze day kings entreaties sell hour beholding considering person picturelike wall renown stir pleased seek danger cruel whence returned brows bound oak sprang joy hearing manchild seeing proved virgilia died madam therein found issue profess sincerely dozen sons alike none less thine eleven nobly voluptuously surfeit gentlewoman lady valeria visit retire myself methinks hither drum pluck hair children shunning stamp cowards got fear born bloody brow mail wiping harvestman task mow lose hire jupiter fool becomes gilt trophy breasts hecuba she suckle hector lovelier forehead spit grecian contemning bid welcome heavens bless lord fell beat knee tread neck ladies both sweet ladyship manifest housekeepers sewing fine spot faith does thank swords schoolmaster father swear boy troth looked wednesday confirmed countenance saw after gilded butterfly caught again over catched fall enraged teeth tear warrant mammocked moods la child crack stitchery play husewife afternoon doors threshold return fie confine unreasonably lies speedy strength prayers save want penelope yarn spun ulysses fill ithaca full moths cambric sensible finger pricking pity pardon truth excellent verily jest last night earnest part nothing prevailing brief excuse obey hereafter disease mirth fare prithee turn thy solemness door yonder wager met horse agreed lie view spoke buy lend hundred years summon town far armies within mile'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1.generate_text(predict_len = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
