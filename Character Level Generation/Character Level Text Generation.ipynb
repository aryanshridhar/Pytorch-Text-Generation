{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "Xy37lJbi3blj",
    "outputId": "d3a1486c-9864-49ed-ad75-3266a6eaa84d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.1+cu101)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch pandas numpy matplotlib nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uwnu1fVb3gHR"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import unidecode\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "EuehMR8T3kv_",
    "outputId": "bf03c643-e7b1-4116-f56a-284f51880ab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l5SKypIM3z0V"
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sb-RVlYM32kr"
   },
   "outputs": [],
   "source": [
    "data = requests.get('https://gist.githubusercontent.com/aryanshridhar/3f90fdf6676ce54b63fd6f165e569ec9/raw/fcf76b36cb75cf4750251bdf4e726807b1d5baa6/Data.txt')\n",
    "data = str(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H08YLC6i39Ek"
   },
   "outputs": [],
   "source": [
    "data = re.sub('\\n' , ' ' , data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tLzU6Ory4BoX",
    "outputId": "2328cd08-dcff-41ed-b414-2e4c8d406182"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e9sKGOPw6h_2"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "OjT0XG-b8Vlv",
    "outputId": "164f8678-0990-4246-eb52-22a95e239e5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\r",
      "\u000b",
      "\f",
      "\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "all_chars = string.printable\n",
    "print(all_chars)\n",
    "print(len(all_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IXO_9bR58Wir"
   },
   "outputs": [],
   "source": [
    "class TextPreprocessing:\n",
    "    def __init__(self , text):\n",
    "        self.text = text\n",
    "        self.text = unidecode.unidecode(self.text)\n",
    "        self.text = re.sub(' +',' ', self.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IZr4YFtx8Zxf"
   },
   "outputs": [],
   "source": [
    "t1 = TextPreprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g80qEQaV8a5I"
   },
   "outputs": [],
   "source": [
    "text = t1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e5HJ4qaH8cKc",
    "outputId": "06d2666a-570a-431e-b9f5-e81ee7a4ec17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "770220"
      ]
     },
     "execution_count": 92,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vTh7Q50m8d2a"
   },
   "outputs": [],
   "source": [
    "class CharGenRNN(nn.Module):\n",
    "    def __init__(self , input_size , hidden_size , output_size , n_layers):\n",
    "        \n",
    "        super(CharGenRNN , self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.input_size, self.hidden_size)\n",
    "        self.rnn = nn.LSTM(self.hidden_size , \n",
    "                           self.hidden_size , \n",
    "                           self.n_layers , \n",
    "                           batch_first = True)\n",
    "        self.linear = nn.Linear(self.hidden_size, self.output_size)\n",
    "   \n",
    "    def forward(self , batch):\n",
    "        batch = batch.long()\n",
    "        out = self.embedding(batch)\n",
    "        out , _ = self.rnn(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIa9si2c8foK"
   },
   "outputs": [],
   "source": [
    "class Generation:\n",
    "    def __init__(self , text , all_chars):\n",
    "        \n",
    "        self.seq_len = 256 # predicting next character form the previous 32 characters\n",
    "        self.batch_size = 126 # total of 16 , 32 seq in a batch\n",
    "        \n",
    "        self.all_chars = {j:i for i,j in enumerate(all_chars)}\n",
    "        self.reverse_chars = {i:j for i,j in enumerate(all_chars)}\n",
    "        \n",
    "        self.text = text[:-(len(text) % (self.seq_len * self.batch_size))]\n",
    "\n",
    "        self.input_size = len(all_chars)\n",
    "        self.hidden_size = 256\n",
    "        self.output_size = self.input_size\n",
    "        self.n_layers = 2\n",
    "        self.epochs = 1000\n",
    "        self.print_every = 10\n",
    "        self.generate_every = 100\n",
    "        self.lr = 0.01\n",
    "        \n",
    "        self.rnn = CharGenRNN(self.input_size , \n",
    "                              self.hidden_size , \n",
    "                              self.output_size , \n",
    "                              self.n_layers).to(device)\n",
    "        \n",
    "\n",
    "    def get_batches(self):\n",
    "        \n",
    "        i = 0\n",
    "        counter = 0    \n",
    "        no_time = len(self.text) // (self.seq_len * self.batch_size)\n",
    "        encoded_text = list(map(lambda x : self.all_chars[x] , self.text))\n",
    "        \n",
    "        while i != int(no_time)-1:\n",
    "            \n",
    "            x_list = []\n",
    "            y_seq = []\n",
    "            \n",
    "            for _ in range(self.batch_size):\n",
    "                x_list.append(encoded_text[counter:self.seq_len+counter])\n",
    "                y_seq.append(encoded_text[counter + 1:self.seq_len+counter+1])\n",
    "                counter += self.seq_len\n",
    "            \n",
    "            i += 1\n",
    "            yield x_list,y_seq\n",
    "            \n",
    "            \n",
    "    def show_batches(self):\n",
    "        for i,j in self.get_batches():\n",
    "            i , j = torch.Tensor(i).long() , torch.Tensor(j).long()\n",
    "            print(i)\n",
    "            print(j)\n",
    "            break\n",
    "            \n",
    "    def init_loss(self):\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            for label,actual in self.get_batches():\n",
    "                    \n",
    "                label = torch.Tensor(label).float().to(device)\n",
    "                actual = torch.Tensor(actual).long().to(device)\n",
    "            \n",
    "                y_pred = self.rnn(label)\n",
    "                y_pred = y_pred.transpose(1,2)\n",
    "                loss = self.criterion(y_pred,actual)\n",
    "                \n",
    "                print(loss.item())\n",
    "                break\n",
    "            break\n",
    "            \n",
    "    def train(self):\n",
    "        \n",
    "        time1 = time.time()\n",
    "        \n",
    "        self.iterations = []\n",
    "        self.loss_ = []\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.rnn.parameters() , lr = self.lr)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            for label,actual in self.get_batches():\n",
    "\n",
    "                    \n",
    "                label = torch.Tensor(label).float().to(device)\n",
    "                actual = torch.Tensor(actual).long().to(device)\n",
    "            \n",
    "                y_pred = self.rnn(label)\n",
    "                y_pred = y_pred.transpose(1,2)\n",
    "                \n",
    "                loss = self.criterion(y_pred,actual)\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "            self.iterations.append(epoch)\n",
    "            self.loss_.append(loss)\n",
    "            \n",
    "            if epoch % self.print_every == 0:\n",
    "                print(f\"Loss after {epoch} iteration : {loss}\")\n",
    "                \n",
    "            if epoch % self.generate_every == 0:\n",
    "                final = time.time() - time1\n",
    "                print(f\"Time elapsed : {round(final , 2)} seconds\")\n",
    "                print(f\"Generated Text after {epoch} epoch : {self.generate_text()}\\n\")\n",
    "                \n",
    "                \n",
    "        self.show_plot(self.iterations , self.loss_)\n",
    "                \n",
    "                \n",
    "    def show_plot(self,x,y):\n",
    "        plt.plot(x,y)\n",
    "        plt.xlabel('Epoch/Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    def generate_text(self , init_str = 'Hello Everybody , I am ',predict_len = 200):\n",
    "        \n",
    "        generated_list = [w for w in init_str]\n",
    "        \n",
    "        for _ in range(predict_len):\n",
    "            \n",
    "            tensor = torch.Tensor([self.all_chars[generated_list[-1].lower()]]).long().unsqueeze(0)\n",
    "            out = self.rnn(tensor.to(device))\n",
    "            prob = F.softmax(out.squeeze() , dim = 0)\n",
    "            \n",
    "            value , ind = torch.topk(prob , 3)\n",
    "            index = np.random.choice(ind.tolist())\n",
    "    \n",
    "            char = self.reverse_chars[index]\n",
    "            \n",
    "            generated_list.append(char)\n",
    "            \n",
    "        return ''.join(generated_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IrJ00Nak8g0H"
   },
   "outputs": [],
   "source": [
    "g1 = Generation(text , all_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "Tg6bcy0Z8iZt",
    "outputId": "04c3d5be-6948-4ab6-dc2e-2e345db636ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[44, 29, 94,  ..., 25, 14, 73],\n",
      "        [94, 18, 29,  ..., 29, 18, 14],\n",
      "        [28, 94, 18,  ..., 15, 10, 12],\n",
      "        ...,\n",
      "        [13, 94, 10,  ..., 28, 14, 10],\n",
      "        [75, 94, 36,  ..., 94, 21, 10],\n",
      "        [27, 16, 14,  ..., 17, 94, 22]])\n",
      "tensor([[29, 94, 32,  ..., 14, 73, 94],\n",
      "        [18, 29, 94,  ..., 18, 14, 28],\n",
      "        [94, 18, 23,  ..., 10, 12, 14],\n",
      "        ...,\n",
      "        [94, 10, 29,  ..., 14, 10, 75],\n",
      "        [94, 36, 94,  ..., 21, 10, 27],\n",
      "        [16, 14, 94,  ..., 94, 22, 18]])\n"
     ]
    }
   ],
   "source": [
    "g1.show_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Tl5uc0g18jRE",
    "outputId": "b552a659-1bcd-43d6-97ff-b7b9059de0a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.605172634124756\n"
     ]
    }
   ],
   "source": [
    "g1.init_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KHWOJyuu8kL1",
    "outputId": "963aff07-e903-455b-ea13-f6652f282059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iteration : 2.937901258468628\n",
      "Time elapsed : 3.97 seconds\n",
      "Generated Text after 0 epoch : Hello Everybody , I am he teenott ateee ht t t a aateeenee hteneneeeeeen a ht t h a h a aat tt htt htee ht ttt aaaa te tt a ttttte htt t aat hteen atte t htt ttt att at a att ateee ate te tteeene hen ttteee ht a te h ht ht \n",
      "\n",
      "Loss after 10 iteration : 1.8760370016098022\n",
      "Loss after 20 iteration : 1.6069384813308716\n",
      "Loss after 30 iteration : 1.4725595712661743\n",
      "Loss after 40 iteration : 1.4013361930847168\n",
      "Loss after 50 iteration : 1.3411526679992676\n",
      "Loss after 60 iteration : 1.3060766458511353\n",
      "Loss after 70 iteration : 1.2838151454925537\n",
      "Loss after 80 iteration : 1.2572195529937744\n",
      "Loss after 90 iteration : 1.2362011671066284\n",
      "Loss after 100 iteration : 1.2250741720199585\n",
      "Time elapsed : 382.86 seconds\n",
      "Generated Text after 100 epoch : Hello Everybody , I am anene athas thedonene to t athist aned,, sen athedoup se stoutonedo t ase thathed,,,\"Llit athist s as s ane sed sened athat and s s s s t aned at s ast s s st tour aned t sthitoure anenend,,\" t at t a\n",
      "\n",
      "Loss after 110 iteration : 1.212751865386963\n",
      "Loss after 120 iteration : 1.1985867023468018\n",
      "Loss after 130 iteration : 1.1896401643753052\n",
      "Loss after 140 iteration : 1.1798182725906372\n",
      "Loss after 150 iteration : 1.165307641029358\n",
      "Loss after 160 iteration : 1.1631282567977905\n",
      "Loss after 170 iteration : 1.1553013324737549\n",
      "Loss after 180 iteration : 1.150071620941162\n",
      "Loss after 190 iteration : 1.1407740116119385\n",
      "Loss after 200 iteration : 1.1365658044815063\n",
      "Time elapsed : 762.06 seconds\n",
      "Generated Text after 200 epoch : Hello Everybody , I am atore out toun tororous,s aned orede tounererounder and tonder that thitoutouthaned an orous on as outhit on touse that oner an tone t on an oun as,sed,\"De atororederounerond,\"D, athised aneroror one \n",
      "\n",
      "Loss after 210 iteration : 1.1270078420639038\n",
      "Loss after 220 iteration : 1.1236011981964111\n",
      "Loss after 230 iteration : 1.1217349767684937\n",
      "Loss after 240 iteration : 1.1166331768035889\n",
      "Loss after 250 iteration : 1.1130988597869873\n",
      "Loss after 260 iteration : 1.1086924076080322\n",
      "Loss after 270 iteration : 1.1049484014511108\n",
      "Loss after 280 iteration : 1.0937870740890503\n",
      "Loss after 290 iteration : 1.0946135520935059\n",
      "Loss after 300 iteration : 1.0939397811889648\n",
      "Time elapsed : 1140.49 seconds\n",
      "Generated Text after 300 epoch : Hello Everybody , I am ouprt anertortherthandon thathit toupr athased toun t ane therthere athit orthised, asedoun toupr ond,se out tounedon an asere ase at thed oupedoupor ortound thered atoune and, toned ortort anedorerer\n",
      "\n",
      "Loss after 310 iteration : 1.0804352760314941\n",
      "Loss after 320 iteration : 1.075188398361206\n",
      "Loss after 330 iteration : 1.073492407798767\n",
      "Loss after 340 iteration : 1.0767507553100586\n",
      "Loss after 350 iteration : 1.0574296712875366\n",
      "Loss after 360 iteration : 1.0580509901046753\n",
      "Loss after 370 iteration : 1.0594277381896973\n",
      "Loss after 380 iteration : 1.058809757232666\n",
      "Loss after 390 iteration : 1.0521152019500732\n",
      "Loss after 400 iteration : 1.0531399250030518\n",
      "Time elapsed : 1520.36 seconds\n",
      "Generated Text after 400 epoch : Hello Everybody , I am atore s,s tortortore at t s,st the thando ton s, ther s ston tonedo athanertore ar thedo aton aredortort t s to andor arer ar sthinert sthartond s t at t athind,st thartherer s,\"Ll tond,s,s are tor at\n",
      "\n",
      "Loss after 410 iteration : 1.0537855625152588\n",
      "Loss after 420 iteration : 1.0419868230819702\n",
      "Loss after 430 iteration : 1.044408917427063\n",
      "Loss after 440 iteration : 1.0399469137191772\n",
      "Loss after 450 iteration : 1.0432530641555786\n",
      "Loss after 460 iteration : 1.0419996976852417\n",
      "Loss after 470 iteration : 1.024526834487915\n",
      "Loss after 480 iteration : 1.0306099653244019\n",
      "Loss after 490 iteration : 1.0273771286010742\n",
      "Loss after 500 iteration : 1.028654932975769\n",
      "Time elapsed : 1897.02 seconds\n",
      "Generated Text after 500 epoch : Hello Everybody , I am arone at s arout s tout at aner ar at at ar se athed,sed,\"Lle s s s, th sed, torout se tondour se thedond,\"Lll t sed,\"Lleredoredoron atorored tor s s,\"D, s,\"D athed,s an t s tone thindond,s, ar ar an \n",
      "\n",
      "Loss after 510 iteration : 1.02653968334198\n",
      "Loss after 520 iteration : 1.0190410614013672\n",
      "Loss after 530 iteration : 1.0233793258666992\n",
      "Loss after 540 iteration : 1.020081877708435\n",
      "Loss after 550 iteration : 1.0148210525512695\n",
      "Loss after 560 iteration : 1.0339224338531494\n",
      "Loss after 570 iteration : 1.0171669721603394\n",
      "Loss after 580 iteration : 1.0148850679397583\n",
      "Loss after 590 iteration : 1.0103139877319336\n",
      "Loss after 600 iteration : 1.0066262483596802\n",
      "Time elapsed : 2276.55 seconds\n",
      "Generated Text after 600 epoch : Hello Everybody , I am ter at t ter. s and as ted,\"Lle sed,\"D,ser ased, se andoredone tedor.is ated,\" and sered s, s s,ser.itedor. ase ate se thit t ath an as the ased ser ser sedorer ate se s, sed,s s, s the s an sere s te\n",
      "\n",
      "Loss after 610 iteration : 1.0109132528305054\n",
      "Loss after 620 iteration : 1.0031541585922241\n",
      "Loss after 630 iteration : 1.002437949180603\n",
      "Loss after 640 iteration : 1.0021007061004639\n",
      "Loss after 650 iteration : 1.0011283159255981\n",
      "Loss after 660 iteration : 0.9965777397155762\n",
      "Loss after 670 iteration : 0.987846851348877\n",
      "Loss after 680 iteration : 0.9923744201660156\n",
      "Loss after 690 iteration : 0.9933195114135742\n",
      "Loss after 700 iteration : 0.9966921210289001\n",
      "Time elapsed : 2654.28 seconds\n",
      "Generated Text after 700 epoch : Hello Everybody , I am sed s t se t tourtoutond,sered, and s, s, touserthed s,se aton s athertoutouse and an th s ase sert and s anerth anerth sedout at as,\" t t at s,s, t s s s s t thedoned,\"Llle toused,s an and t atored a\n",
      "\n",
      "Loss after 710 iteration : 0.9978668689727783\n",
      "Loss after 720 iteration : 0.990939736366272\n",
      "Loss after 730 iteration : 0.983587920665741\n",
      "Loss after 740 iteration : 0.9859175086021423\n",
      "Loss after 750 iteration : 0.999564528465271\n",
      "Loss after 760 iteration : 0.9814989566802979\n",
      "Loss after 770 iteration : 0.982889711856842\n",
      "Loss after 780 iteration : 0.9794160723686218\n",
      "Loss after 790 iteration : 0.9841965436935425\n",
      "Loss after 800 iteration : 0.9886448383331299\n",
      "Time elapsed : 3032.76 seconds\n",
      "Generated Text after 800 epoch : Hello Everybody , I am atond,s,\" anerthis, me atore mande aned aned mone mout ther tond, at mouthert athit tored ar at matoutone artheded an thed, t t aner mert me t mat at thede matouprert me t mer morer manere marert an a\n",
      "\n",
      "Loss after 810 iteration : 0.9891473650932312\n",
      "Loss after 820 iteration : 0.9861191511154175\n",
      "Loss after 830 iteration : 0.983976423740387\n",
      "Loss after 840 iteration : 0.9768779277801514\n",
      "Loss after 850 iteration : 0.9768356084823608\n",
      "Loss after 860 iteration : 0.9647527933120728\n",
      "Loss after 870 iteration : 0.9725438356399536\n",
      "Loss after 880 iteration : 0.9660179018974304\n",
      "Loss after 890 iteration : 0.9710487723350525\n",
      "Loss after 900 iteration : 0.9666891098022461\n",
      "Time elapsed : 3411.27 seconds\n",
      "Generated Text after 900 epoch : Hello Everybody , I am t t thartored t t toutouthart toned,sed, onere t t ar the tourer orthedouthan t ar touton touned,s,\"D,sert out the ort ond,sedound,\"Dorer the or on the thered toure t tonered,\"D arere athanerthathando\n",
      "\n",
      "Loss after 910 iteration : 0.9818451404571533\n",
      "Loss after 920 iteration : 0.9756617546081543\n",
      "Loss after 930 iteration : 0.9702174663543701\n",
      "Loss after 940 iteration : 0.9595872759819031\n",
      "Loss after 950 iteration : 0.9661054611206055\n",
      "Loss after 960 iteration : 0.9692056775093079\n",
      "Loss after 970 iteration : 0.9609841704368591\n",
      "Loss after 980 iteration : 0.97291499376297\n",
      "Loss after 990 iteration : 0.977693498134613\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "colab_type": "code",
    "id": "HpThgJK68nDw",
    "outputId": "6be388d9-5893-4f9d-bda3-655e9f6dabc1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'Hello Everybody , I am thane t on an athis,se onere ore oned on ore tore touthas oner as,\"D atoure t at and torthit orere as,sed athed, as ane t ous ator orer as onert t t oner anedous thathis, athas tous as toredor tondor thed, t tortond ased, aton on ored,se on orert ase on one orthe t andouthas,s,\"D tond ous,\" out an one athat thathis ous, tout thiner at thinered,sere thit ond, athere athandore outor orthind,s,\" out thise on ourtous, and, at aser outourtout t an anere andousedorthed as or andor t an anered oned,\" orthere outortorer tone t ond tonerertoredortond,\"Ld,\" ond,sed, t ore ous, asert an or orer t as ator t thasere ortor an ored at thiser tondouthe ort ton one ored,\"D thiser t t tort our ous,se orered, at ased outhandond than ous, orthan at ase that andortort ane as and ase aserer t t aser orer ase oner t t t ored, t at at t and,s aned,\" ase and,se onedoure t t and ator as t tone at at t outoured tor thathase tourthe t an thas,\" t ort atortone ase as ond,\"Lllld on tone thatored orered, ous,s,\"D t '"
      ]
     },
     "execution_count": 105,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1.generate_text(predict_len = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QytVplztCfbX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Character Level Text Generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
